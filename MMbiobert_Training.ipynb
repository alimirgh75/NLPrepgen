{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install pytorch_lightning\n","!pip install transformers\n","!pip install pretrainedmodels"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:34.891901Z","iopub.status.busy":"2022-04-14T06:56:34.891518Z","iopub.status.idle":"2022-04-14T06:56:43.754659Z","shell.execute_reply":"2022-04-14T06:56:43.753848Z","shell.execute_reply.started":"2022-04-14T06:56:34.891866Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import random\n","import math\n","import cv2\n","\n","import torch\n","from torchvision import transforms, models\n","from torch.cuda.amp import GradScaler\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torch.nn.functional as F\n","#from transformers import BertTokenizer, BertModel\n","from transformers import AutoTokenizer, AutoModel\n","\n","from nltk.translate.bleu_score import sentence_bleu\n","from tqdm import tqdm\n","from PIL import Image\n","from random import choice\n","import matplotlib.pyplot as plt\n","\n","import pretrainedmodels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.757113Z","iopub.status.busy":"2022-04-14T06:56:43.75592Z","iopub.status.idle":"2022-04-14T06:56:43.763887Z","shell.execute_reply":"2022-04-14T06:56:43.762879Z","shell.execute_reply.started":"2022-04-14T06:56:43.757059Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    random.seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"markdown","metadata":{},"source":["## get data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.766551Z","iopub.status.busy":"2022-04-14T06:56:43.766216Z","iopub.status.idle":"2022-04-14T06:56:43.773681Z","shell.execute_reply":"2022-04-14T06:56:43.773026Z","shell.execute_reply.started":"2022-04-14T06:56:43.766495Z"},"trusted":true},"outputs":[],"source":["def make_df(file_path):\n","    paths = os.listdir(file_path)\n","    \n","    df_list = []\n","    \n","    for p in paths:\n","        df = pd.read_csv(os.path.join(file_path, p), sep='|', names = ['img_id', 'question', 'answer'])\n","        df['category'] = p.split('_')[1]\n","        df['mode'] = p.split('_')[2][:-4]\n","        df_list.append(df)\n","    \n","    return pd.concat(df_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.775502Z","iopub.status.busy":"2022-04-14T06:56:43.775011Z","iopub.status.idle":"2022-04-14T06:56:43.798719Z","shell.execute_reply":"2022-04-14T06:56:43.797897Z","shell.execute_reply.started":"2022-04-14T06:56:43.775467Z"},"trusted":true},"outputs":[],"source":["def load_all_data(args, remove = None):\n","    \n","    #### 2019 ####\n","    clef2019train_path = '/hdd2/datasets/medvqa_kai_datasets/clef2019/ImageClef-2019-VQA-Med-Training'\n","    clef2019valid_path = '/hdd2/datasets/medvqa_kai_datasets/clef2019/ImageClef-2019-VQA-Med-Validation'\n","    clef2019test_path = '/hdd2/datasets/medvqa_kai_datasets/clef2019/ImageClef-2019-VQA-Med-Test'\n","    \n","    traindf2019 = pd.read_csv(os.path.join(clef2019train_path, 'traindf.csv'))\n","    valdf2019 = pd.read_csv(os.path.join(clef2019valid_path, 'valdf.csv'))\n","    testdf2019 = pd.read_csv(os.path.join(clef2019test_path, 'testdf.csv'))\n","\n","    if remove is not None:\n","        traindf2019 = traindf2019[~traindf2019['img_id'].isin(remove)].reset_index(drop=True)\n","\n","    traindf2019['img_id'] = traindf2019['img_id'].apply(lambda x: os.path.join(clef2019train_path, 'Train_images', x + '.jpg'))\n","    valdf2019['img_id'] = valdf2019['img_id'].apply(lambda x: os.path.join(clef2019valid_path, 'Val_images', x + '.jpg'))\n","    testdf2019['img_id'] = testdf2019['img_id'].apply(lambda x: os.path.join(clef2019test_path, 'Test_images', x + '.jpg'))\n","    # testdf2019['img_id'] = testdf2019['img_id'].apply(lambda x: os.path.join(args.data_dir2019, x + '.jpg'))\n","\n","    traindf2019['category'] = traindf2019['category'].str.lower()\n","    valdf2019['category'] = valdf2019['category'].str.lower()\n","    testdf2019['category'] = testdf2019['category'].str.lower()\n","\n","    traindf2019['answer'] = traindf2019['answer'].str.lower()\n","    valdf2019['answer'] = valdf2019['answer'].str.lower()\n","    testdf2019['answer'] = testdf2019['answer'].str.lower()\n","\n","    traindf2019 = traindf2019.sample(frac = args.train_pct)\n","    valdf2019 = valdf2019.sample(frac = args.valid_pct)\n","    testdf2019 = testdf2019.sample(frac = args.test_pct)\n","    \n","    #### 2020 ####\n","    clef2020train_path = '/hdd2/datasets/medvqa_kai_datasets/clef2020/VQA-Med-2020-Task1-VQAnswering-TrainVal-Sets/VQAMed2020-VQAnswering-TrainingSet'\n","    clef2020valid_path = '/hdd2/datasets/medvqa_kai_datasets/clef2020/VQA-Med-2020-Task1-VQAnswering-TrainVal-Sets/VQAMed2020-VQAnswering-ValidationSet'\n","    \n","    traindf2020 = pd.read_csv(os.path.join(clef2020train_path, 'clef2020_train_category.csv'))\n","    valdf2020 = pd.read_csv(os.path.join(clef2020valid_path, 'clef2020_valid_category.csv'))\n","\n","    traindf2020['img_id'] = traindf2020['img_id'].apply(lambda x: os.path.join(clef2020train_path, 'VQAnswering_2020_Train_images', x + '.jpg'))\n","    valdf2020['img_id'] = valdf2020['img_id'].apply(lambda x: os.path.join(clef2020valid_path, 'VQAnswering_2020_Val_images', x + '.jpg'))\n","\n","    traindf2020['category'] = traindf2020['category'].str.lower()\n","    valdf2020['category'] = valdf2020['category'].str.lower()\n","\n","    traindf2020['answer'] = traindf2020['answer'].str.lower()\n","    valdf2020['answer'] = valdf2020['answer'].str.lower()\n","\n","    traindf2020 = traindf2020.sample(frac = args.train_pct)\n","    valdf2020 = valdf2020.sample(frac = args.valid_pct)\n","    \n","    #### 2018 ####\n","    clef2018train_path = '/hdd2/datasets/medvqa_kai_datasets/clef2018/VQAMed2018Train/VQAMed2018Train'\n","    clef2018valid_path = '/hdd2/datasets/medvqa_kai_datasets/clef2018/VQAMed2018Valid/VQAMed2018Valid'\n","    \n","    traindf2018 = pd.read_csv(os.path.join(clef2018train_path, 'clef2018_train_category.csv'))\n","    valdf2018 = pd.read_csv(os.path.join(clef2018valid_path, 'clef2018_valid_category.csv'))\n","\n","    traindf2018['img_id'] = traindf2018['img_id'].apply(lambda x: os.path.join(clef2018train_path, 'VQAMed2018Train-images', x + '.jpg'))\n","    valdf2018['img_id'] = valdf2018['img_id'].apply(lambda x: os.path.join(clef2018valid_path, 'VQAMed2018Valid-images', x + '.jpg'))\n","\n","    traindf2018['category'] = traindf2018['category'].str.lower()\n","    valdf2018['category'] = valdf2018['category'].str.lower()\n","\n","    traindf2018['answer'] = traindf2018['answer'].str.lower()\n","    valdf2018['answer'] = valdf2018['answer'].str.lower()\n","\n","    traindf2018 = traindf2018.sample(frac = args.train_pct)\n","    valdf2018 = valdf2018.sample(frac = args.valid_pct)\n","    \n","    #### VQARAD ####\n","    vqaradtrain_path='/hdd2/datasets/medvqa_kai_datasets/vqarad'\n","    traindf_vqarad = pd.read_csv(os.path.join(vqaradtrain_path, 'vqa_rad.csv'))\n","\n","    traindf_vqarad['img_id'] = traindf_vqarad['img_id'].apply(lambda x: os.path.join(vqaradtrain_path, 'VQA_RAD Image Folder', x + '.jpg'))\n","    \n","    traindf_vqarad.head()\n","    \n","    #### union ####\n","    \n","    data_frames_train = [traindf2018, traindf2019, traindf2020, traindf_vqarad]\n","    data_frames_valid = [valdf2018, valdf2019, valdf2020]\n","    \n","    union_train = pd.concat(data_frames_train, ignore_index=True)\n","    union_valid = pd.concat(data_frames_valid, ignore_index=True)\n","    \n","    return union_train, union_valid, testdf2019"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.801686Z","iopub.status.busy":"2022-04-14T06:56:43.801429Z","iopub.status.idle":"2022-04-14T06:56:43.812396Z","shell.execute_reply":"2022-04-14T06:56:43.811423Z","shell.execute_reply.started":"2022-04-14T06:56:43.801653Z"},"trusted":true},"outputs":[],"source":["def gelu(x):\n","    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.813786Z","iopub.status.busy":"2022-04-14T06:56:43.813485Z","iopub.status.idle":"2022-04-14T06:56:43.823785Z","shell.execute_reply":"2022-04-14T06:56:43.823127Z","shell.execute_reply.started":"2022-04-14T06:56:43.813752Z"},"trusted":true},"outputs":[],"source":["def encode_text(caption,tokenizer, args):\n","    part1 = [0 for _ in range(5)]\n","    #get token ids and remove [CLS] and [SEP] token id\n","    part2 = tokenizer.encode(caption)[1:-1]\n","\n","    tokens = [tokenizer.cls_token_id] + part1 + [tokenizer.sep_token_id] + part2[:args.max_position_embeddings-8] + [tokenizer.sep_token_id]\n","    segment_ids = [0]*(len(part1)+2) + [1]*(len(part2[:args.max_position_embeddings-8])+1)\n","    input_mask = [1]*len(tokens)\n","    n_pad = args.max_position_embeddings - len(tokens)\n","    tokens.extend([0]*n_pad)\n","    segment_ids.extend([0]*n_pad)\n","    input_mask.extend([0]*n_pad)\n","\n","    \n","    return tokens, segment_ids, input_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.825563Z","iopub.status.busy":"2022-04-14T06:56:43.82537Z","iopub.status.idle":"2022-04-14T06:56:43.833126Z","shell.execute_reply":"2022-04-14T06:56:43.832409Z","shell.execute_reply.started":"2022-04-14T06:56:43.825541Z"},"trusted":true},"outputs":[],"source":["def onehot(size, target):\n","    vec = torch.zeros(size, dtype=torch.float32)\n","    vec[target] = 1.\n","    return vec"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.83501Z","iopub.status.busy":"2022-04-14T06:56:43.834498Z","iopub.status.idle":"2022-04-14T06:56:43.84413Z","shell.execute_reply":"2022-04-14T06:56:43.843412Z","shell.execute_reply.started":"2022-04-14T06:56:43.834975Z"},"trusted":true},"outputs":[],"source":["class LabelSmoothing(nn.Module):\n","    def __init__(self, smoothing = 0.1):\n","        super(LabelSmoothing, self).__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","\n","    def forward(self, x, target):\n","        if self.training:\n","            x = x.float()\n","            target = target.float()\n","            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n","\n","            nll_loss = -logprobs * target\n","            nll_loss = nll_loss.sum(-1)\n","    \n","            smooth_loss = -logprobs.mean(dim=-1)\n","\n","            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n","\n","            return loss.mean()\n","        else:\n","            return torch.nn.functional.cross_entropy(x, target)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.847839Z","iopub.status.busy":"2022-04-14T06:56:43.847543Z","iopub.status.idle":"2022-04-14T06:56:43.854215Z","shell.execute_reply":"2022-04-14T06:56:43.85355Z","shell.execute_reply.started":"2022-04-14T06:56:43.847813Z"},"trusted":true},"outputs":[],"source":["def crop(img):\n","    c_y, c_x = img.shape[:2]\n","    c_y = c_y // 2\n","    c_x = c_x // 2\n","    shorter = min(img.shape[:2])\n","    if img.shape[0] <= img.shape[1]:\n","        img = img[c_y - shorter // 2: c_y + (shorter - shorter // 2) - 20, c_x - shorter // 2: c_x + (shorter - shorter // 2), :]\n","    else:\n","        img = img[c_y - shorter // 2: c_y + (shorter - shorter // 2), c_x - shorter // 2: c_x + (shorter - shorter // 2), :]\n","    \n","    return img"]},{"cell_type":"markdown","metadata":{},"source":["## Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:43.856308Z","iopub.status.busy":"2022-04-14T06:56:43.855499Z","iopub.status.idle":"2022-04-14T06:56:44.032596Z","shell.execute_reply":"2022-04-14T06:56:44.031795Z","shell.execute_reply.started":"2022-04-14T06:56:43.856268Z"},"trusted":true},"outputs":[],"source":["class VQAMed(Dataset):\n","    def __init__(self, df, imgsize, tfm, args, mode = 'train'):\n","        self.df = df\n","        self.tfm = tfm\n","        self.size = imgsize\n","        self.args = args\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        path = self.df.loc[idx,'img_id']\n","        question = self.df.loc[idx, 'question']\n"," \n","        answer = self.df.loc[idx, 'answer']\n","\n","        if self.mode == 'eval':\n","            tok_ques = self.tokenizer.tokenize(question)\n","\n","        if self.args.smoothing:\n","            answer = onehot(self.args.num_classes, answer)\n","\n","        img = cv2.imread(path)\n","  \n","\n","        if self.tfm:\n","            img = self.tfm(img)\n","            \n","        tokens, segment_ids, input_mask= encode_text(question, self.tokenizer, self.args)\n","        \n","\n","\n","        return img, torch.tensor(tokens, dtype = torch.long), torch.tensor(segment_ids, dtype = torch.long), torch.tensor(input_mask, dtype = torch.long), torch.tensor(answer, dtype = torch.long), path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.03441Z","iopub.status.busy":"2022-04-14T06:56:44.033712Z","iopub.status.idle":"2022-04-14T06:56:44.045752Z","shell.execute_reply":"2022-04-14T06:56:44.045117Z","shell.execute_reply.started":"2022-04-14T06:56:44.03437Z"},"trusted":true},"outputs":[],"source":["class Model_Keyword(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Model_Keyword, self).__init__()\n","        self.model = pretrainedmodels.__dict__['se_resnext50_32x4d'](num_classes=1000, pretrained='imagenet')\n","        last_in = self.model.last_linear.in_features\n","        self.model.last_linear = nn.Identity()\n","        self.embed = nn.Embedding(3, last_in)\n","        self.last_layer = nn.Linear(2 * last_in, num_classes)\n","\n","    def forward(self, img, keyword):\n","\n","        img_feat = self.model(img)\n","        key_feat = self.embed(keyword)\n","\n","        feat = torch.cat([img_feat, key_feat], -1)\n","\n","        logits = self.last_layer(feat)\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.047171Z","iopub.status.busy":"2022-04-14T06:56:44.046882Z","iopub.status.idle":"2022-04-14T06:56:44.054947Z","shell.execute_reply":"2022-04-14T06:56:44.054288Z","shell.execute_reply.started":"2022-04-14T06:56:44.047129Z"},"trusted":true},"outputs":[],"source":["def calculate_bleu_score(preds,targets, idx2ans):\n","  bleu_per_answer = np.asarray([sentence_bleu([idx2ans[target].split()],idx2ans[pred].split(), weights = [1]) for pred,target in zip(preds,targets)])\n","  return np.mean(bleu_per_answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.05756Z","iopub.status.busy":"2022-04-14T06:56:44.056963Z","iopub.status.idle":"2022-04-14T06:56:44.068733Z","shell.execute_reply":"2022-04-14T06:56:44.068102Z","shell.execute_reply.started":"2022-04-14T06:56:44.057526Z"},"trusted":true},"outputs":[],"source":["class Embeddings(nn.Module):\n","    def __init__(self, args):\n","        super(Embeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(args.vocab_size, 128, padding_idx=0)\n","        self.word_embeddings_2 = nn.Linear(128, args.hidden_size, bias=False)\n","        self.position_embeddings = nn.Embedding(args.max_position_embeddings, args.hidden_size)\n","        self.type_embeddings = nn.Embedding(3, args.hidden_size)\n","        self.LayerNorm = nn.LayerNorm(args.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n","        self.len = args.max_position_embeddings\n","    def forward(self, input_ids, segment_ids, position_ids=None):\n","        if position_ids is None:\n","            if torch.cuda.is_available():\n","                position_ids = torch.arange(self.len, dtype=torch.long).cuda()\n","            else:\n","                position_ids = torch.arange(self.len, dtype=torch.long)\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        words_embeddings = self.word_embeddings(input_ids)\n","        words_embeddings = self.word_embeddings_2(words_embeddings)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        token_type_embeddings = self.type_embeddings(segment_ids)\n","        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","\n","        return embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.070422Z","iopub.status.busy":"2022-04-14T06:56:44.070027Z","iopub.status.idle":"2022-04-14T06:56:44.100646Z","shell.execute_reply":"2022-04-14T06:56:44.099909Z","shell.execute_reply.started":"2022-04-14T06:56:44.070386Z"},"trusted":true},"outputs":[],"source":["class Transfer(nn.Module):\n","    def __init__(self,args):\n","        super(Transfer, self).__init__()\n","\n","        self.args = args\n","        self.num_vis = args.num_vis\n","        self.model = models.resnet152(pretrained=True)\n","        # for p in self.parameters():\n","        #     p.requires_grad=False\n","\n","        if self.num_vis == 5:\n","            self.relu = nn.ReLU()\n","            self.conv2 = nn.Conv2d(2048, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap2 = nn.AdaptiveAvgPool2d((1,1))\n","            self.conv3 = nn.Conv2d(1024, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap3 = nn.AdaptiveAvgPool2d((1,1))\n","            self.conv4 = nn.Conv2d(512, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap4 = nn.AdaptiveAvgPool2d((1,1))\n","            self.conv5 = nn.Conv2d(256, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap5 = nn.AdaptiveAvgPool2d((1,1))\n","            self.conv7 = nn.Conv2d(64, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap7 = nn.AdaptiveAvgPool2d((1,1))\n","\n","        elif self.num_vis == 3:\n","            self.relu = nn.ReLU()\n","            self.conv2 = nn.Conv2d(2048, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap2 = nn.AdaptiveAvgPool2d((1,1))\n","            self.conv3 = nn.Conv2d(1024, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap3 = nn.AdaptiveAvgPool2d((1,1))\n","            self.conv4 = nn.Conv2d(512, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap4 = nn.AdaptiveAvgPool2d((1,1))\n","\n","        else:\n","            self.relu = nn.ReLU()\n","            self.conv2 = nn.Conv2d(2048, args.hidden_size, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            self.gap2 = nn.AdaptiveAvgPool2d((1,1))            \n","            \n","    def forward(self, img):\n","\n","        if self.num_vis == 5: \n","            modules2 = list(self.model.children())[:-2]\n","            fix2 = nn.Sequential(*modules2)\n","            inter_2 = self.conv2(fix2(img))\n","            v_2 = self.gap2(self.relu(inter_2)).view(-1,self.args.hidden_size)\n","            modules3 = list(self.model.children())[:-3]\n","            fix3 = nn.Sequential(*modules3)\n","            inter_3 = self.conv3(fix3(img))\n","            v_3 = self.gap3(self.relu(inter_3)).view(-1,self.args.hidden_size)\n","            modules4 = list(self.model.children())[:-4]\n","            fix4 = nn.Sequential(*modules4)\n","            inter_4 = self.conv4(fix4(img))\n","            v_4 = self.gap4(self.relu(inter_4)).view(-1,self.args.hidden_size)\n","            modules5 = list(self.model.children())[:-5]\n","            fix5 = nn.Sequential(*modules5)\n","            inter_5 = self.conv5(fix5(img))\n","            v_5 = self.gap5(self.relu(inter_5)).view(-1,self.args.hidden_size)\n","            modules7 = list(self.model.children())[:-7]\n","            fix7 = nn.Sequential(*modules7)\n","            inter_7 = self.conv7(fix7(img))\n","            v_7 = self.gap7(self.relu(inter_7)).view(-1,self.args.hidden_size)\n","\n","            return v_2, v_3, v_4, v_5, v_7, [inter_2.mean(1), inter_3.mean(1), inter_4.mean(1), inter_5.mean(1), inter_7.mean(1)]\n","\n","        if self.num_vis == 3: \n","            modules2 = list(self.model.children())[:-2]\n","            fix2 = nn.Sequential(*modules2)\n","            inter_2 = self.conv2(fix2(img))\n","            v_2 = self.gap2(self.relu(inter_2)).view(-1,self.args.hidden_size)\n","            modules3 = list(self.model.children())[:-3]\n","            fix3 = nn.Sequential(*modules3)\n","            inter_3 = self.conv3(fix3(img))\n","            v_3 = self.gap3(self.relu(inter_3)).view(-1,self.args.hidden_size)\n","            modules4 = list(self.model.children())[:-4]\n","            fix4 = nn.Sequential(*modules4)\n","            inter_4 = self.conv4(fix4(img))\n","            v_4 = self.gap4(self.relu(inter_4)).view(-1,self.args.hidden_size)\n","\n","            return v_2, v_3, v_4, [inter_2.mean(1), inter_3.mean(1), inter_4.mean(1)]\n","\n","        else:\n","            modules2 = list(self.model.children())[:-2]\n","            fix2 = nn.Sequential(*modules2)\n","            inter_2 = self.conv2(fix2(img))\n","            v_2 = self.gap2(self.relu(inter_2)).view(-1,self.args.hidden_size)    \n","            \n","            return v_2, [inter_2.mean(1)]  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.10342Z","iopub.status.busy":"2022-04-14T06:56:44.102885Z","iopub.status.idle":"2022-04-14T06:56:44.117303Z","shell.execute_reply":"2022-04-14T06:56:44.116598Z","shell.execute_reply.started":"2022-04-14T06:56:44.103382Z"},"trusted":true},"outputs":[],"source":["class MultiHeadedSelfAttention(nn.Module):\n","    def __init__(self,args):\n","        super(MultiHeadedSelfAttention,self).__init__()\n","        self.proj_q = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.proj_k = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.proj_v = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.drop = nn.Dropout(args.hidden_dropout_prob)\n","        self.scores = None\n","        self.n_heads = args.heads\n","    def forward(self, x, mask):\n","        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n","        q, k, v = (self.split_last(x, (self.n_heads, -1)).transpose(1, 2) for x in [q, k, v])\n","        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n","        if mask is not None:\n","            mask = mask[:, None, None, :].float()\n","            scores -= 10000.0 * (1.0 - mask)\n","        scores = self.drop(F.softmax(scores, dim=-1))\n","        h = (scores @ v).transpose(1, 2).contiguous()\n","        h = self.merge_last(h, 2)\n","        self.scores = scores\n","        return h, scores\n","    def split_last(self, x, shape):\n","        shape = list(shape)\n","        assert shape.count(-1) <= 1  \n","        if -1 in shape:\n","            shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n","        return x.view(*x.size()[:-1], *shape)\n","    def merge_last(self, x, n_dims):\n","        s = x.size()\n","        assert n_dims > 1 and n_dims < len(s)\n","        return x.view(*s[:-n_dims], -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.119028Z","iopub.status.busy":"2022-04-14T06:56:44.118754Z","iopub.status.idle":"2022-04-14T06:56:44.130016Z","shell.execute_reply":"2022-04-14T06:56:44.129226Z","shell.execute_reply.started":"2022-04-14T06:56:44.118996Z"},"trusted":true},"outputs":[],"source":["class PositionWiseFeedForward(nn.Module):\n","    def __init__(self,args):\n","        super(PositionWiseFeedForward,self).__init__()\n","        self.fc1 = nn.Linear(args.hidden_size, args.hidden_size*4)\n","        self.fc2 = nn.Linear(args.hidden_size*4, args.hidden_size)\n","    def forward(self, x):\n","        return self.fc2(gelu(self.fc1(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.133045Z","iopub.status.busy":"2022-04-14T06:56:44.132844Z","iopub.status.idle":"2022-04-14T06:56:44.151687Z","shell.execute_reply":"2022-04-14T06:56:44.150954Z","shell.execute_reply.started":"2022-04-14T06:56:44.133015Z"},"trusted":true},"outputs":[],"source":["class BertLayer(nn.Module):\n","    def __init__(self,args, share='all', norm='pre'):\n","        super(BertLayer, self).__init__()\n","        self.share = share\n","        self.norm_pos = norm\n","        self.norm1 = nn.LayerNorm(args.hidden_size, eps=1e-12)\n","        self.norm2 = nn.LayerNorm(args.hidden_size, eps=1e-12)\n","        self.drop1 = nn.Dropout(args.hidden_dropout_prob)\n","        self.drop2 = nn.Dropout(args.hidden_dropout_prob)\n","        if self.share == 'ffn':\n","            self.attention = nn.ModuleList([MultiHeadedSelfAttention(args) for _ in range(args.n_layers)])\n","            self.proj = nn.ModuleList([nn.Linear(args.hidden_size, args.hidden_size) for _ in range(args.n_layers)])\n","            self.feedforward = PositionWiseFeedForward(args)\n","        elif self.share == 'att':\n","            self.attention = MultiHeadedSelfAttention(args)\n","            self.proj = nn.Linear(args.hidden_size, args.hidden_size)\n","            self.feedforward = nn.ModuleList([PositionWiseFeedForward(args) for _ in range(args.n_layers)])\n","        elif self.share == 'all':\n","            self.attention = MultiHeadedSelfAttention(args)\n","            self.proj = nn.Linear(args.hidden_size, args.hidden_size)\n","            self.feedforward = PositionWiseFeedForward(args)\n","        elif self.share == 'none':\n","            self.attention = nn.ModuleList([MultiHeadedSelfAttention(args) for _ in range(args.n_layers)])\n","            self.proj = nn.ModuleList([nn.Linear(args.hidden_size, args.hidden_size) for _ in range(args.n_layers)])\n","            self.feedforward = nn.ModuleList([PositionWiseFeedForward(args) for _ in range(args.n_layers)])\n","    def forward(self, hidden_states, attention_mask, layer_num):\n","        if self.norm_pos == 'pre':\n","            if isinstance(self.attention, nn.ModuleList):\n","                attn_output, attn_scores = self.attention[layer_num](self.norm1(hidden_states), attention_mask)\n","                h = self.proj[layer_num](attn_output)\n","            else:\n","                h = self.proj(self.attention(self.norm1(hidden_states), attention_mask))\n","            out = hidden_states + self.drop1(h)\n","            if isinstance(self.feedforward, nn.ModuleList):\n","                h = self.feedforward[layer_num](self.norm1(out))\n","            else:\n","                h = self.feedforward(self.norm1(out))\n","            out = out + self.drop2(h)\n","        if self.norm_pos == 'post':\n","            if isinstance(self.attention, nn.ModuleList):\n","                h = self.proj[layer_num](self.attention[layer_num](hidden_states, attention_mask))\n","            else:\n","                h = self.proj(self.attention(hidden_states, attention_mask))\n","            out = self.norm1(hidden_states + self.drop1(h))\n","            if isinstance(self.feedforward, nn.ModuleList):\n","                h = self.feedforward[layer_num](out)\n","            else:\n","                h = self.feedforward(out)\n","            out = self.norm2(out + self.drop2(h))\n","        return out, attn_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.153303Z","iopub.status.busy":"2022-04-14T06:56:44.152949Z","iopub.status.idle":"2022-04-14T06:56:44.171692Z","shell.execute_reply":"2022-04-14T06:56:44.171018Z","shell.execute_reply.started":"2022-04-14T06:56:44.153269Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, args):\n","        super(Transformer,self).__init__()\n","        base_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","        bert_model = nn.Sequential(*list(base_model.children())[0:])\n","        self.bert_embedding = bert_model[0]\n","        # self.embed = Embeddings(args)\n","        self.num_vis = args.num_vis\n","        self.trans = Transfer(args)\n","        self.blocks = BertLayer(args,share='none', norm='pre')\n","        self.n_layers = args.n_layers\n","        \n","    def forward(self, img, input_ids, token_type_ids, mask):\n","\n","        if self.num_vis==5:\n","            #print(\"img.shape: \" ,img.shape)\n","            v_2, v_3, v_4, v_5, v_7, intermediate = self.trans(img)\n","        elif self.num_vis==3:\n","            v_2, v_3, v_4, intermediate = self.trans(img)\n","        else:\n","            v_2, intermediate = self.trans(img)\n","        # h = self.embed(input_ids, token_type_ids)\n","        h = self.bert_embedding(input_ids=input_ids, token_type_ids=token_type_ids, position_ids=None)\n","        #print(\"h.size: \" ,h.shape)\n","        #print(\"v_2.size: \" ,v_2.shape)\n","        #print(\"v_3.size: \" ,v_3.shape)\n","        #print(\"v_4.size: \" ,v_4.shape)\n","        #print(\"v_5.size: \" ,v_5.shape)\n","        #print(\"v_7.size: \" ,v_7.shape)\n","        if self.num_vis == 5:\n","            for i in range(len(h)):\n","                h[i][1] = v_2[i]\n","            for i in range(len(h)):\n","                h[i][2] = v_3[i]\n","            for i in range(len(h)):\n","                h[i][3] = v_4[i]\n","            for i in range(len(h)):\n","                h[i][4] = v_5[i]\n","            for i in range(len(h)):\n","                h[i][5] = v_7[i]\n","\n","        elif self.num_vis == 3:\n","            for i in range(len(h)):\n","                h[i][1] = v_2[i]\n","            for i in range(len(h)):\n","                h[i][2] = v_3[i]\n","            for i in range(len(h)):\n","                h[i][3] = v_4[i]\n","\n","        else:\n","            for i in range(len(h)):\n","                h[i][1] = v_2[i]\n","\n","\n","        hidden_states = []\n","        all_attn_scores = []\n","        for i in range(self.n_layers):\n","            h, attn_scores = self.blocks(h, mask, i)\n","            hidden_states.append(h)\n","            all_attn_scores.append(attn_scores)\n","\n","        return torch.stack(hidden_states, 0), torch.stack(all_attn_scores, 0), intermediate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.175215Z","iopub.status.busy":"2022-04-14T06:56:44.174974Z","iopub.status.idle":"2022-04-14T06:56:44.183943Z","shell.execute_reply":"2022-04-14T06:56:44.183288Z","shell.execute_reply.started":"2022-04-14T06:56:44.175182Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self,args):\n","        super(Model,self).__init__()\n","        self.args = args\n","        self.transformer = Transformer(args)\n","        self.fc1 = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.activ1 = nn.Tanh()\n","        self.classifier = nn.Sequential(nn.Linear(args.hidden_size, args.hidden_size),\n","                                        nn.LayerNorm(args.hidden_size, eps=1e-12, elementwise_affine=True),\n","                                        nn.Linear(args.hidden_size, args.vocab_size))\n","    def forward(self, img, input_ids, segment_ids, input_mask):\n","        h, attn_scores, intermediate = self.transformer(img, input_ids, segment_ids, input_mask)\n","        pooled_h = self.activ1(self.fc1(h[-1].mean(1)))\n","        logits = self.classifier(pooled_h)\n","        return logits, attn_scores, intermediate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.187206Z","iopub.status.busy":"2022-04-14T06:56:44.186827Z","iopub.status.idle":"2022-04-14T06:56:44.203328Z","shell.execute_reply":"2022-04-14T06:56:44.202671Z","shell.execute_reply.started":"2022-04-14T06:56:44.187181Z"},"trusted":true},"outputs":[],"source":["def train_one_epoch(loader, model, optimizer, criterion, device, scaler, args, idx2ans):\n","\n","    model.train()\n","    train_loss = []\n","    IMGIDS = []\n","    PREDS = []\n","    TARGETS = []\n","    bar = tqdm(loader, leave = False)\n","    for (img, question_token,segment_ids,attention_mask,target, imgid) in bar:\n","        \n","        img, question_token,segment_ids,attention_mask,target = img.to(device), question_token.to(device), segment_ids.to(device), attention_mask.to(device), target.to(device)\n","        question_token = question_token.squeeze(1)\n","        attention_mask = attention_mask.squeeze(1)\n","        loss_func = criterion\n","        optimizer.zero_grad()\n","\n","        if args.mixed_precision:\n","            with torch.cuda.amp.autocast(): \n","                logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n","                loss = loss_func(logits, target)\n","        else:\n","            logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n","            loss = loss_func(logits, target)\n","\n","        if args.mixed_precision:\n","            scaler.scale(loss)\n","            loss.backward()\n","\n","            if args.clip:\n","                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","        else:\n","            loss.backward()\n","\n","            if args.clip:\n","                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","                \n","            optimizer.step()\n","\n","        if args.smoothing:\n","            TARGETS.append(target.argmax(1))\n","        else:\n","            TARGETS.append(target)    \n","\n","        pred = logits.softmax(1).argmax(1).detach()\n","        PREDS.append(pred)\n","        IMGIDS.append(imgid)\n","\n","        loss_np = loss.detach().cpu().numpy()\n","        train_loss.append(loss_np)\n","        bar.set_description('train_loss: %.5f' % (loss_np))\n","\n","    PREDS = torch.cat(PREDS).cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","    IMGIDS = [i for sub in IMGIDS for i in sub]\n","\n","    acc = (PREDS == TARGETS).mean() * 100.\n","    bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n","\n","    return np.mean(train_loss), PREDS, acc, bleu, IMGIDS"]},{"cell_type":"markdown","metadata":{},"source":["## eval methods"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.205472Z","iopub.status.busy":"2022-04-14T06:56:44.204447Z","iopub.status.idle":"2022-04-14T06:56:44.227559Z","shell.execute_reply":"2022-04-14T06:56:44.226747Z","shell.execute_reply.started":"2022-04-14T06:56:44.205435Z"},"trusted":true},"outputs":[],"source":["def validate(loader, model, criterion, device, scaler, args, val_df, idx2ans):\n","\n","    model.eval()\n","    val_loss = []\n","\n","    PREDS = []\n","    TARGETS = []\n","    bar = tqdm(loader, leave=False)\n","\n","    with torch.no_grad():\n","        for (img, question_token,segment_ids,attention_mask,target, _) in bar:\n","\n","            img, question_token,segment_ids,attention_mask,target = img.to(device), question_token.to(device), segment_ids.to(device), attention_mask.to(device), target.to(device)\n","            question_token = question_token.squeeze(1)\n","            attention_mask = attention_mask.squeeze(1)\n","\n","\n","            if args.mixed_precision:\n","                with torch.cuda.amp.autocast(): \n","                    logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n","                    loss = criterion(logits, target)\n","            else:\n","                logits, _ , _= model(img, question_token, segment_ids, attention_mask)\n","                loss = criterion(logits, target)\n","\n","\n","            loss_np = loss.detach().cpu().numpy()\n","\n","            pred = logits.softmax(1).argmax(1).detach()\n","\n","            PREDS.append(pred)\n","\n","            if args.smoothing:\n","                TARGETS.append(target.argmax(1))\n","            else:\n","                TARGETS.append(target)\n","\n","            val_loss.append(loss_np)\n","\n","            bar.set_description('val_loss: %.5f' % (loss_np))\n","\n","        val_loss = np.mean(val_loss)\n","\n","    PREDS = torch.cat(PREDS).cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","\n","    # Calculate total and category wise accuracy\n","    if args.category:\n","        acc = (PREDS == TARGETS).mean() * 100.\n","        bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n","    else:\n","        total_acc = (PREDS == TARGETS).mean() * 100.\n","        binary_acc = (PREDS[val_df['category']=='binary'] == TARGETS[val_df['category']=='binary']).mean() * 100.\n","        plane_acc = (PREDS[val_df['category']=='plane'] == TARGETS[val_df['category']=='plane']).mean() * 100.\n","        organ_acc = (PREDS[val_df['category']=='organ'] == TARGETS[val_df['category']=='organ']).mean() * 100.\n","        modality_acc = (PREDS[val_df['category']=='modality'] == TARGETS[val_df['category']=='modality']).mean() * 100.\n","        abnorm_acc = (PREDS[val_df['category']=='abnormality'] == TARGETS[val_df['category']=='abnormality']).mean() * 100.\n","\n","        acc = {'val_total_acc': np.round(total_acc, 4), 'val_binary_acc': np.round(binary_acc, 4), 'val_plane_acc': np.round(plane_acc, 4), 'val_organ_acc': np.round(organ_acc, 4), \n","               'val_modality_acc': np.round(modality_acc, 4), 'val_abnorm_acc': np.round(abnorm_acc, 4)}\n","\n","        # add bleu score code\n","        total_bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n","        plane_bleu = calculate_bleu_score(PREDS[val_df['category']=='plane'],TARGETS[val_df['category']=='plane'],idx2ans)\n","        binary_bleu = calculate_bleu_score(PREDS[val_df['category']=='binary'],TARGETS[val_df['category']=='binary'],idx2ans)\n","        organ_bleu = calculate_bleu_score(PREDS[val_df['category']=='organ'],TARGETS[val_df['category']=='organ'],idx2ans)\n","        modality_bleu = calculate_bleu_score(PREDS[val_df['category']=='modality'],TARGETS[val_df['category']=='modality'],idx2ans)\n","        abnorm_bleu = calculate_bleu_score(PREDS[val_df['category']=='abnormality'],TARGETS[val_df['category']=='abnormality'],idx2ans)\n","\n","\n","        bleu = {'val_total_bleu': np.round(total_bleu, 4), 'val_binary_bleu': np.round(binary_bleu, 4), 'val_plane_bleu': np.round(plane_bleu, 4), 'val_organ_bleu': np.round(organ_bleu, 4), \n","            'val_modality_bleu': np.round(modality_bleu, 4), 'val_abnorm_bleu': np.round(abnorm_bleu, 4)}\n","\n","    return val_loss, PREDS, acc, bleu  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.229362Z","iopub.status.busy":"2022-04-14T06:56:44.228689Z","iopub.status.idle":"2022-04-14T06:56:44.251674Z","shell.execute_reply":"2022-04-14T06:56:44.250963Z","shell.execute_reply.started":"2022-04-14T06:56:44.229329Z"},"trusted":true},"outputs":[],"source":["def test(loader, model, criterion, device, scaler, args, val_df,idx2ans):\n","\n","    model.eval()\n","    TARGETS = []\n","    PREDS = []\n","    test_loss = []\n","\n","    with torch.no_grad():\n","        for (img,question_token,segment_ids,attention_mask,target, _) in tqdm(loader, leave=False):\n","\n","            img, question_token, segment_ids, attention_mask, target = img.to(device), question_token.to(device), segment_ids.to(device), attention_mask.to(device), target.to(device)\n","            question_token = question_token.squeeze(1)\n","            attention_mask = attention_mask.squeeze(1)\n","            \n","            if args.mixed_precision:\n","                with torch.cuda.amp.autocast(): \n","                    logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n","                    loss = criterion(logits, target)\n","            else:\n","                logits, _, _ = model(img, question_token, segment_ids, attention_mask)\n","                loss = criterion(logits, target)\n","\n","\n","            loss_np = loss.detach().cpu().numpy()\n","\n","            test_loss.append(loss_np)\n","\n","            pred = logits.softmax(1).argmax(1).detach()\n","            \n","            PREDS.append(pred)\n","\n","            if args.smoothing:\n","                TARGETS.append(target.argmax(1))\n","            else:\n","                TARGETS.append(target)\n","\n","        test_loss = np.mean(test_loss)\n","\n","    PREDS = torch.cat(PREDS).cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","\n","    if args.category:\n","        acc = (PREDS == TARGETS).mean() * 100.\n","        bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n","    else:\n","        total_acc = (PREDS == TARGETS).mean() * 100.\n","        binary_acc = (PREDS[val_df['category']=='binary'] == TARGETS[val_df['category']=='binary']).mean() * 100.\n","        plane_acc = (PREDS[val_df['category']=='plane'] == TARGETS[val_df['category']=='plane']).mean() * 100.\n","        organ_acc = (PREDS[val_df['category']=='organ'] == TARGETS[val_df['category']=='organ']).mean() * 100.\n","        modality_acc = (PREDS[val_df['category']=='modality'] == TARGETS[val_df['category']=='modality']).mean() * 100.\n","        abnorm_acc = (PREDS[val_df['category']=='abnormality'] == TARGETS[val_df['category']=='abnormality']).mean() * 100.\n","\n","        acc = {'total_acc': np.round(total_acc, 4), 'binary_acc': np.round(binary_acc, 4), 'plane_acc': np.round(plane_acc, 4), 'organ_acc': np.round(organ_acc, 4), \n","               'modality_acc': np.round(modality_acc, 4), 'abnorm_acc': np.round(abnorm_acc, 4)}\n","\n","        # add bleu score code\n","        total_bleu = calculate_bleu_score(PREDS,TARGETS,idx2ans)\n","        binary_bleu = calculate_bleu_score(PREDS[val_df['category']=='binary'],TARGETS[val_df['category']=='binary'],idx2ans)\n","        plane_bleu = calculate_bleu_score(PREDS[val_df['category']=='plane'],TARGETS[val_df['category']=='plane'],idx2ans)\n","        organ_bleu = calculate_bleu_score(PREDS[val_df['category']=='organ'],TARGETS[val_df['category']=='organ'],idx2ans)\n","        modality_bleu = calculate_bleu_score(PREDS[val_df['category']=='modality'],TARGETS[val_df['category']=='modality'],idx2ans)\n","        abnorm_bleu = calculate_bleu_score(PREDS[val_df['category']=='abnormality'],TARGETS[val_df['category']=='abnormality'],idx2ans)\n","\n","\n","        bleu = {'total_bleu': np.round(total_bleu, 4),  'binary_bleu': np.round(binary_bleu, 4), 'plane_bleu': np.round(plane_bleu, 4), 'organ_bleu': np.round(organ_bleu, 4), \n","            'modality_bleu': np.round(modality_bleu, 4), 'abnorm_bleu': np.round(abnorm_bleu, 4)}\n","\n","\n","    return test_loss, PREDS, acc, bleu"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:44.253526Z","iopub.status.busy":"2022-04-14T06:56:44.253044Z","iopub.status.idle":"2022-04-14T06:56:45.096105Z","shell.execute_reply":"2022-04-14T06:56:45.095379Z","shell.execute_reply.started":"2022-04-14T06:56:44.25349Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import sys\n","#from utils import seed_everything, Model, VQAMed, train_one_epoch, validate, test, load_data, LabelSmoothing, train_img_only, val_img_only, test_img_only\n","#import wandb\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torchvision import transforms, models\n","from torch.cuda.amp import GradScaler\n","import os\n","import warnings\n","import albumentations as A\n","import pretrainedmodels\n","from albumentations.core.composition import OneOf\n","#from albumentations.pytorch.transforms import ToTensorV2\n","\n","warnings.simplefilter(\"ignore\", UserWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:45.099371Z","iopub.status.busy":"2022-04-14T06:56:45.099164Z","iopub.status.idle":"2022-04-14T06:56:45.121193Z","shell.execute_reply":"2022-04-14T06:56:45.120541Z","shell.execute_reply.started":"2022-04-14T06:56:45.099341Z"},"trusted":true},"outputs":[],"source":["sys.argv = ['-f']\n","\n","parser = argparse.ArgumentParser(description = \"Finetune on ImageClef 2019\")\n","\n","parser.add_argument('--run_name', type = str, required = False, default = \"MMBioBERT_allclef_vqarad_pre\", help = \"run name for wandb\")\n","#parser.add_argument('--data_dir', type = str, required = False, default = \"../input/clef2019/clef2019\", help = \"path for data\")\n","#parser.add_argument('--model_dir', type = str, required = False, default = \"../input/mmbiobert-weights/MMBERT_clef2019_BioBERT_pretrained_acc(1).pt\", help = \"path to load weights\")\n","#parser.add_argument('--model_dir', type = str, required = False, default = \"../input/mmbert-pretrain-roco-weights/rocopretrain_weights.pt\", help = \"path to load weights\")\n","#parser.add_argument('--save_dir', type = str, required = False, default = \"/content/drive/MyDrive/Colab Notebooks/Thesis/Transformer VQA/MMBERT weights/clef2019_BioBERT\", help = \"path to save weights\")\n","parser.add_argument('--category', type = str, required = False, default = None,  help = \"choose specific category if you want\")\n","parser.add_argument('--use_pretrained', action = 'store_true', default = False, help = \"use pretrained weights or not\")\n","parser.add_argument('--mixed_precision', action = 'store_true', default = False, help = \"use mixed precision or not\")\n","parser.add_argument('--clip', action = 'store_true', default = False, help = \"clip the gradients or not\")\n","\n","parser.add_argument('--seed', type = int, required = False, default = 42, help = \"set seed for reproducibility\")\n","parser.add_argument('--num_workers', type = int, required = False, default = 4, help = \"number of workers\")\n","parser.add_argument('--epochs', type = int, required = False, default = 100, help = \"num epochs to train\")\n","parser.add_argument('--train_pct', type = float, required = False, default = 1.0, help = \"fraction of train samples to select\")\n","parser.add_argument('--valid_pct', type = float, required = False, default = 1.0, help = \"fraction of validation samples to select\")\n","parser.add_argument('--test_pct', type = float, required = False, default = 1.0, help = \"fraction of test samples to select\")\n","\n","parser.add_argument('--max_position_embeddings', type = int, required = False, default = 28, help = \"max length of sequence\")\n","parser.add_argument('--batch_size', type = int, required = False, default = 10, help = \"batch size\")\n","parser.add_argument('--lr', type = float, required = False, default = 1e-4, help = \"learning rate'\")\n","# parser.add_argument('--weight_decay', type = float, required = False, default = 1e-2, help = \" weight decay for gradients\")\n","parser.add_argument('--factor', type = float, required = False, default = 0.1, help = \"factor for rlp\")\n","parser.add_argument('--patience', type = int, required = False, default = 10, help = \"patience for rlp\")\n","# parser.add_argument('--lr_min', type = float, required = False, default = 1e-6, help = \"minimum lr for Cosine Annealing\")\n","parser.add_argument('--hidden_dropout_prob', type = float, required = False, default = 0.3, help = \"hidden dropout probability\")\n","parser.add_argument('--smoothing', type = float, required = False, default = None, help = \"label smoothing\")\n","\n","parser.add_argument('--image_size', type = int, required = False, default = 224, help = \"image size\")\n","parser.add_argument('--hidden_size', type = int, required = False, default = 768, help = \"hidden size\") #og 312\n","parser.add_argument('--vocab_size', type = int, required = False, default = 30522, help = \"vocab size\")\n","parser.add_argument('--type_vocab_size', type = int, required = False, default = 2, help = \"type vocab size\")\n","parser.add_argument('--heads', type = int, required = False, default = 12, help = \"heads\")\n","parser.add_argument('--n_layers', type = int, required = False, default = 4, help = \"num of layers\")\n","parser.add_argument('--num_vis', type = int, required = False , default = 5, help = \"num of visual embeddings\") #num of conv2d Layers in the transformer, can be: 5, 3 or 1\n","\n","args = parser.parse_args()\n","\n","#wandb.init(project='medvqa', name = args.run_name, config = args)\n","\n","seed_everything(args.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:45.122828Z","iopub.status.busy":"2022-04-14T06:56:45.122512Z","iopub.status.idle":"2022-04-14T06:56:45.434666Z","shell.execute_reply":"2022-04-14T06:56:45.433985Z","shell.execute_reply.started":"2022-04-14T06:56:45.122793Z"},"trusted":true},"outputs":[],"source":["train_df, val_df, test_df = load_all_data(args)\n","print('len(train_df): ' ,len(train_df))\n","print('len(val_df): ' ,len(val_df))\n","print('len(test_df): ' ,len(test_df))\n","\n","if args.category:\n","        \n","    train_df = train_df[train_df['category']==args.category].reset_index(drop=True)\n","    val_df = val_df[val_df['category']==args.category].reset_index(drop=True)\n","    test_df = test_df[test_df['category']==args.category].reset_index(drop=True)\n","\n","    train_df = train_df[~train_df['answer'].isin(['yes', 'no'])].reset_index(drop = True)\n","    val_df = val_df[~val_df['answer'].isin(['yes', 'no'])].reset_index(drop = True)\n","    test_df = test_df[~test_df['answer'].isin(['yes', 'no'])].reset_index(drop = True)\n","\n","df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n","\n","ans2idx = {ans:idx for idx,ans in enumerate(df['answer'].unique())}\n","idx2ans = {idx:ans for ans,idx in ans2idx.items()}\n","df['answer'] = df['answer'].map(ans2idx).astype(int)\n","train_df = df[df['mode']=='train'].reset_index(drop=True)\n","val_df = df[df['mode']=='val'].reset_index(drop=True)\n","test_df = df[df['mode']=='test'].reset_index(drop=True)\n","\n","num_classes = len(ans2idx)\n","\n","args.num_classes = num_classes\n","print(num_classes)\n","\n","df.head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:56:45.436747Z","iopub.status.busy":"2022-04-14T06:56:45.436171Z","iopub.status.idle":"2022-04-14T06:57:56.534577Z","shell.execute_reply":"2022-04-14T06:57:56.533908Z","shell.execute_reply.started":"2022-04-14T06:56:45.436707Z"},"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model = Model(args)\n","\n","if args.use_pretrained:\n","    print(\"loading weights\")\n","    model.load_state_dict(torch.load(args.model_dir))\n","    print(\"loaded weights\")\n","\n","model.classifier[2] = nn.Linear(args.hidden_size, num_classes)\n","\n","model.to(device)\n","\n","#wandb.watch(model, log='all')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:57:56.538946Z","iopub.status.busy":"2022-04-14T06:57:56.538741Z","iopub.status.idle":"2022-04-14T06:57:56.552761Z","shell.execute_reply":"2022-04-14T06:57:56.550491Z","shell.execute_reply.started":"2022-04-14T06:57:56.538921Z"},"trusted":true},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(),lr=args.lr)\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience = args.patience, factor = args.factor, verbose = True)\n","\n","if args.smoothing:\n","    criterion = LabelSmoothing(smoothing=args.smoothing)\n","else:\n","    criterion = nn.CrossEntropyLoss()\n","\n","scaler = GradScaler()\n","\n","\n","train_tfm = transforms.Compose([transforms.ToPILImage(),\n","                                transforms.RandomResizedCrop(224,scale=(0.75,1.25),ratio=(0.75,1.25)),\n","                                transforms.RandomRotation(10),\n","                                # Cutout(),\n","                                transforms.ColorJitter(brightness=0.4,contrast=0.4,saturation=0.4,hue=0.4),\n","                                transforms.ToTensor(), \n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","val_tfm = transforms.Compose([transforms.ToPILImage(),\n","                              transforms.Resize((224,224)),\n","                              transforms.ToTensor(), \n","                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","test_tfm = transforms.Compose([transforms.ToPILImage(),\n","                               transforms.Resize((224,224)),    \n","                               transforms.ToTensor(), \n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:57:56.555851Z","iopub.status.busy":"2022-04-14T06:57:56.555642Z","iopub.status.idle":"2022-04-14T06:58:11.519556Z","shell.execute_reply":"2022-04-14T06:58:11.518851Z","shell.execute_reply.started":"2022-04-14T06:57:56.555825Z"},"trusted":true},"outputs":[],"source":["traindataset = VQAMed(train_df, imgsize = args.image_size, tfm = train_tfm, args = args)\n","valdataset = VQAMed(val_df, imgsize = args.image_size, tfm = val_tfm, args = args)\n","testdataset = VQAMed(test_df, imgsize = args.image_size, tfm = test_tfm, args = args)\n","\n","trainloader = DataLoader(traindataset, batch_size = args.batch_size, shuffle=True, num_workers = args.num_workers)\n","valloader = DataLoader(valdataset, batch_size = args.batch_size, shuffle=False, num_workers = args.num_workers)\n","testloader = DataLoader(testdataset, batch_size = args.batch_size, shuffle=False, num_workers = args.num_workers)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:58:11.520887Z","iopub.status.busy":"2022-04-14T06:58:11.520655Z","iopub.status.idle":"2022-04-14T06:58:11.527239Z","shell.execute_reply":"2022-04-14T06:58:11.526508Z","shell.execute_reply.started":"2022-04-14T06:58:11.520853Z"},"trusted":true},"outputs":[],"source":["from datetime import datetime\n","now = datetime.now()\n","\n","#with open(\"../input/mmbert-pretrained/MMBERT_allclef_pre.txt\") as input_txt:\n","#    with open(\"MMBERT_allclef_pre.txt\", \"w\") as f:\n","#        for line in input_txt:\n","#            f.write(line) \n","\n","f = open(f'{args.run_name}.txt', \"w\")\n","f.write(\"datasets used: clef2018, clef2019, clef2020, vqa-rad\")\n","f.write(\"no pretrain\")\n","f.write(\"\\n\\n\\nMMBERT training \" + str(now))\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:58:11.529014Z","iopub.status.busy":"2022-04-14T06:58:11.52856Z","iopub.status.idle":"2022-04-14T06:58:11.540341Z","shell.execute_reply":"2022-04-14T06:58:11.539536Z","shell.execute_reply.started":"2022-04-14T06:58:11.528978Z"},"trusted":true},"outputs":[],"source":["best_acc = 0\n","best_loss = np.inf\n","counter = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-14T06:58:11.541789Z","iopub.status.busy":"2022-04-14T06:58:11.541542Z","iopub.status.idle":"2022-04-14T10:58:41.116295Z","shell.execute_reply":"2022-04-14T10:58:41.114742Z","shell.execute_reply.started":"2022-04-14T06:58:11.541755Z"},"trusted":true},"outputs":[],"source":["for epoch in range(args.epochs):\n","\n","    print(f'Epoch {epoch+1}/{args.epochs}')\n","\n","\n","    train_loss, _, train_acc, _, _ = train_one_epoch(trainloader, model, optimizer, criterion, device, scaler, args, idx2ans)\n","    val_loss, val_predictions, val_acc, val_bleu = validate(valloader, model, criterion, device, scaler, args, val_df,idx2ans)\n","    test_loss, test_predictions, test_acc, test_bleu = test(testloader, model, criterion, device, scaler, args, test_df,idx2ans)\n","\n","    scheduler.step(val_loss)\n","\n","    print(\"val_loss: \" ,val_loss)\n","    print(\"val_acc: \" ,val_acc)\n","    print(\"test_acc: \" ,test_acc)\n","    \n","    f = open(f'{args.run_name}.txt', \"a\")\n","    f.write('\\n\\nepoch ' + str(epoch))\n","    f.write('\\nAccuracy and Loss')\n","    f.write('\\ntrain_acc: ' + str(train_acc) + '   train_loss: ' + str(train_loss) + ',')\n","    f.write('\\nval_acc: ' + str(val_acc) + '   val_loss: ' + str(val_loss) + ',')\n","    f.write('\\ntest_acc: ' + str(test_acc) + '   test_loss: ' + str(test_loss) + ',')\n","    f.write('\\nBLEU validation: ' + str(val_bleu))\n","    f.write('\\nBLEU test: ' + str(test_bleu))\n","    f.write('\\nlearning_rate: ' + str(optimizer.param_groups[0][\"lr\"]))\n","\n","    if test_acc['total_acc'] > best_acc:\n","        print('Saving model best acc')\n","        f.write('\\nnew best test total acc')\n","        torch.save(model.state_dict(), f'{args.run_name}_bestacc.pt')\n","        best_acc=test_acc['total_acc']\n","    \n","    if val_loss < best_loss:\n","        print('Saving model best val loss')\n","        f.write('\\nnew best val_loss')\n","        torch.save(model.state_dict(), f'{args.run_name}.pt')\n","        best_loss=val_loss\n","        counter=0\n","        f.write('\\ncounter: ' + str(counter))\n","    else:\n","        counter+=1\n","        print(\"counter: \" ,counter)\n","        f.write('\\ncounter: ' + str(counter))\n","        if counter > 20:\n","            break\n","            \n","    f.close()"]}],"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.6.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}
